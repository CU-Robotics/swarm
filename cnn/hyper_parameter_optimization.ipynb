{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11393,
     "status": "ok",
     "timestamp": 1761443855249,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "rQ2omtClxb4k",
    "outputId": "05b28b98-5083-4b60-ab07-4f76a94be1f2"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CU-Robotics/swarm/blob/main/cnn/hyper_parameter_optimization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12220,
     "status": "ok",
     "timestamp": 1761443867471,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "rPyiT2xNsZ_7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab (likely VS Code or local environment)\n"
     ]
    }
   ],
   "source": [
    "if 'COLAB_GPU' in os.environ or 'TENSORFLOW_USE_SYNC_ON_FINISH' in os.environ:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Download the zip file from Google Drive\n",
    "    !gdown --fuzzy https://drive.google.com/file/d/1yyGVx6jvZgLLSX4yx9c8JBKDM5bTa9eK/view?usp=drive_link\n",
    "    !unzip -q data.zip -d /content/data/\n",
    "    !pip install optuna\n",
    "    on_colab = True\n",
    "\n",
    "else:\n",
    "    print(\"Not running in Google Colab (likely VS Code or local environment)\")\n",
    "    on_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761443867525,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "A7P_6CRfsiPa",
    "outputId": "9e517c1c-476d-40c9-c49f-88dbc3d09135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "DEVICE = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 7\n",
    "\n",
    "if on_colab:\n",
    "    DIR = os.getcwd() + \"/data/\"\n",
    "else:\n",
    "    DIR = os.getcwd() + \"/../collections/data/\"\n",
    "    # import your own data :p\n",
    "\n",
    "EPOCHS = 10\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1761443874294,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "9JTvgm45WnkN"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class for loading images and labels\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = json.load(open(annotations_file)) # path to pipeline.json\n",
    "        self.img_dir = img_dir                              # folder where the cleaned images are\n",
    "        self.transform = transform\n",
    "        self.classes = {'1':1, '2':2, '3':3, '4':4, 'sentry':5, 'base':6, 'tower':7}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    # gets image and label at index idx based on position in json\n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.img_labels[idx]\n",
    "        img_name = image_data[\"name\"]\n",
    "        img_folder = image_data[\"folder\"]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, img_folder, \"cropped\", img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label = self.classes[image_data[\"labels\"][\"icon\"]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1761443874298,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "TOFJ1JZjXCmY"
   },
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 1\n",
    "    for i in range(num_layers):\n",
    "        out_features = trial.suggest_int(f'n_units_l{i}', 16, 64, step = 16)\n",
    "        kernel_size = trial.suggest_int(f'kernel_size_l{i}', 3, 7, step=2)\n",
    "        layers.append(nn.Conv2d(in_features, out_features, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(2))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add_module(\"flatten\", nn.Flatten())\n",
    "\n",
    "    # Estimate feature size after convolutions\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.zeros(1, 1, 100, 100)\n",
    "        n_features = model(dummy).shape[1]\n",
    "\n",
    "    # Add final classifier\n",
    "    model.add_module(\"fc\", nn.Linear(n_features, CLASSES))\n",
    "    model.add_module(\"logsoftmax\", nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1761443874302,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "tbOQrmxmsj5J"
   },
   "outputs": [],
   "source": [
    "def get_data_loaders():\n",
    "    annotations_file = os.path.join(DIR, 'cleaned_metadata.json')\n",
    "\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Grayscale(num_output_channels=1),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Lambda(lambda t: t.sqrt()),\n",
    "    # ])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "      transforms.Grayscale(num_output_channels=1),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.RandomRotation(15),\n",
    "      transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Lambda(lambda t: t.sqrt()),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(annotations_file, img_dir=DIR, transform=transform)\n",
    "\n",
    "    # Split sizes\n",
    "    train_size = int(0.8 * len(dataset))  # 80%\n",
    "    val_size = len(dataset) - train_size  # remaining 20%\n",
    "\n",
    "    # Random split\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1761443874323,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "EqoHcM8Hsmm1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Suggest hyperparameters for optimizer\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader, valid_loader = get_data_loaders()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data, target = data.view(data.size(0), 1, 100, 100).to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "                data, target = data.view(data.size(0), 1, 100, 100).to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1011235,
     "status": "ok",
     "timestamp": 1761444885561,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "ipsIX0EgxPBc",
    "outputId": "c3164379-4649-408a-b26b-37ec61b6a3c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 16:12:26,860] A new study created in memory with name: no-name-bc9f26c4-6a7a-47e1-8379-359aa7163ab5\n",
      "[W 2025-10-28 16:12:26,873] Trial 0 failed with parameters: {'num_layers': 1, 'n_units_l0': 48, 'kernel_size_l0': 3, 'lr': 3.7766934659193476e-05, 'optimizer': 'Adam'} because of the following error: FileNotFoundError(2, 'No such file or directory').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\geodz\\AppData\\Local\\Temp\\ipykernel_28908\\2896430027.py\", line 9, in objective\n",
      "    train_loader, valid_loader = get_data_loaders()\n",
      "                                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\geodz\\AppData\\Local\\Temp\\ipykernel_28908\\3575368090.py\", line 19, in get_data_loaders\n",
      "    dataset = CustomImageDataset(annotations_file, img_dir=DIR, transform=transform)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\geodz\\AppData\\Local\\Temp\\ipykernel_28908\\433251058.py\", line 4, in __init__\n",
      "    self.img_labels = json.load(open(annotations_file)) # path to pipeline.json\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 343, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\Users\\\\geodz\\\\OneDrive\\\\Documents\\\\School\\\\Robotics Club\\\\swarm\\\\cnn/../collections/data/cleaned_metadata.json'\n",
      "[W 2025-10-28 16:12:26,875] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\geodz\\\\OneDrive\\\\Documents\\\\School\\\\Robotics Club\\\\swarm\\\\cnn/../collections/data/cleaned_metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m pruned_trials = study.get_trials(deepcopy=\u001b[38;5;28;01mFalse\u001b[39;00m, states=[TrialState.PRUNED])\n\u001b[32m      5\u001b[39m complete_trials = study.get_trials(deepcopy=\u001b[38;5;28;01mFalse\u001b[39;00m, states=[TrialState.COMPLETE])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      6\u001b[39m optimizer_name = trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m, [\u001b[33m'\u001b[39m\u001b[33mAdam\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRMSprop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSGD\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m optimizer = \u001b[38;5;28mgetattr\u001b[39m(optim, optimizer_name)(model.parameters(), lr=lr)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m train_loader, valid_loader = \u001b[43mget_data_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Training of the model.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mget_data_loaders\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# transform = transforms.Compose([\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#     transforms.Grayscale(num_output_channels=1),\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     transforms.ToTensor(),\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     transforms.Lambda(lambda t: t.sqrt()),\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ])\u001b[39;00m\n\u001b[32m     10\u001b[39m transform = transforms.Compose([\n\u001b[32m     11\u001b[39m   transforms.Grayscale(num_output_channels=\u001b[32m1\u001b[39m),\n\u001b[32m     12\u001b[39m   transforms.RandomHorizontalFlip(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m   transforms.Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m t: t.sqrt()),\n\u001b[32m     17\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m dataset = \u001b[43mCustomImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Split sizes\u001b[39;00m\n\u001b[32m     22\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(dataset))  \u001b[38;5;66;03m# 80%\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mCustomImageDataset.__init__\u001b[39m\u001b[34m(self, annotations_file, img_dir, transform)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotations_file, img_dir, transform=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.img_labels = json.load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# path to pipeline.json\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.img_dir = img_dir                              \u001b[38;5;66;03m# folder where the cleaned images are\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.transform = transform\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\geodz\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\geodz\\\\OneDrive\\\\Documents\\\\School\\\\Robotics Club\\\\swarm\\\\cnn/../collections/data/cleaned_metadata.json'"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=1000)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1761444885849,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "1q4j2khX397-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert trials to DataFrame\n",
    "df = study.trials_dataframe()\n",
    "# print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"optuna_trials_rand.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDnFMg90QyL3/H0zRsHI00",
   "gpuType": "T4",
   "mount_file_id": "1xJmP1lHd-OxgeB-glZY4UZ1VX0fjSeNu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
