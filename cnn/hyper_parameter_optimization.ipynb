{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11393,
     "status": "ok",
     "timestamp": 1761443855249,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "rQ2omtClxb4k",
    "outputId": "05b28b98-5083-4b60-ab07-4f76a94be1f2"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CU-Robotics/swarm/blob/main/cnn/hyper_parameter_optimization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12220,
     "status": "ok",
     "timestamp": 1761443867471,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "rPyiT2xNsZ_7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import gdown\n",
    "import zipfile\n",
    "import gdown\n",
    "\n",
    "if 'COLAB_GPU' in os.environ or 'TENSORFLOW_USE_SYNC_ON_FINISH' in os.environ:\n",
    "    print(\"Running in Google Colab\")\n",
    "    !pip install optuna\n",
    "    \n",
    "    zip_path = os.getcwd() + \"/data.zip\"\n",
    "    extract_path = os.getcwd() + \"/data/\"\n",
    "    \n",
    "    from google.colab import userdata\n",
    "    data_link = userdata.get(\"DATA_FILE_LINK\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running in Google Colab (likely VS Code or local environment)\")\n",
    "    \n",
    "    zip_path = os.getcwd() + \"/../collections/data.zip\"\n",
    "    extract_path = os.getcwd() + \"/../collections/data/\"\n",
    "\n",
    "    load_dotenv()\n",
    "    data_link = os.getenv(\"DATA_FILE_LINK\")\n",
    "\n",
    "# Construct download URL\n",
    "url = f\"https://drive.google.com/uc?id={data_link}\"\n",
    "\n",
    "# Download the zip file\n",
    "if not os.path.exists(zip_path):\n",
    "    gdown.download(url, zip_path, quiet=False)\n",
    "\n",
    "# Unzip it\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Data extracted to {extract_path}\")\n",
    "os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761443867525,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "A7P_6CRfsiPa",
    "outputId": "9e517c1c-476d-40c9-c49f-88dbc3d09135"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "DEVICE = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 7\n",
    "EPOCHS = 10\n",
    "DIR = extract_path\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1761443874294,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "9JTvgm45WnkN"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class for loading images and labels\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = json.load(open(annotations_file)) # path to pipeline.json\n",
    "        self.img_dir = img_dir                              # folder where the cleaned images are\n",
    "        self.transform = transform\n",
    "        self.classes = {'1':1, '2':2, '3':3, '4':4, 'sentry':5, 'base':6, 'tower':7}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    # gets image and label at index idx based on position in json\n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.img_labels[idx]\n",
    "        img_name = image_data[\"name\"]\n",
    "        img_folder = image_data[\"folder\"]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, img_folder, \"cropped\", img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label = self.classes[image_data[\"labels\"][\"icon\"]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1761443874298,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "TOFJ1JZjXCmY"
   },
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 1\n",
    "    for i in range(num_layers):\n",
    "        out_features = trial.suggest_int(f'n_units_l{i}', 16, 64, step = 16)\n",
    "        kernel_size = trial.suggest_int(f'kernel_size_l{i}', 3, 7, step=2)\n",
    "        layers.append(nn.Conv2d(in_features, out_features, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(2))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add_module(\"flatten\", nn.Flatten())\n",
    "\n",
    "    # Estimate feature size after convolutions\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.zeros(1, 1, 100, 100)\n",
    "        n_features = model(dummy).shape[1]\n",
    "\n",
    "    # Add final classifier\n",
    "    model.add_module(\"fc\", nn.Linear(n_features, CLASSES))\n",
    "    model.add_module(\"logsoftmax\", nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1761443874302,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "tbOQrmxmsj5J"
   },
   "outputs": [],
   "source": [
    "def get_data_loaders():\n",
    "    annotations_file = os.path.join(DIR, 'cleaned_metadata.json')\n",
    "\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Grayscale(num_output_channels=1),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Lambda(lambda t: t.sqrt()),\n",
    "    # ])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "      transforms.Grayscale(num_output_channels=1),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.RandomRotation(15),\n",
    "      transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Lambda(lambda t: t.sqrt()),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(annotations_file, img_dir=DIR, transform=transform)\n",
    "\n",
    "    # Split sizes\n",
    "    train_size = int(0.8 * len(dataset))  # 80%\n",
    "    val_size = len(dataset) - train_size  # remaining 20%\n",
    "\n",
    "    # Random split\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1761443874323,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "EqoHcM8Hsmm1"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Suggest hyperparameters for optimizer\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader, valid_loader = get_data_loaders()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data, target = data.view(data.size(0), 1, 100, 100).to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "                data, target = data.view(data.size(0), 1, 100, 100).to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1011235,
     "status": "ok",
     "timestamp": 1761444885561,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "ipsIX0EgxPBc",
    "outputId": "c3164379-4649-408a-b26b-37ec61b6a3c7"
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=1000)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1761444885849,
     "user": {
      "displayName": "Daniel Zhu",
      "userId": "00161032685532082979"
     },
     "user_tz": 360
    },
    "id": "1q4j2khX397-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert trials to DataFrame\n",
    "df = study.trials_dataframe()\n",
    "# print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"optuna_trials_rand.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best model to train on full training set and evaluate on test set\n",
    "best_model = define_model(study.best_trial).to(DEVICE)\n",
    "print(study.best_params)\n",
    "\n",
    "dataset = CustomImageDataset(os.path.join(DIR, 'cleaned_metadata.json'), img_dir=DIR, transform=transforms.Compose([\n",
    "      transforms.Grayscale(num_output_channels=1),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Lambda(lambda t: t.sqrt()),\n",
    "    ]))\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "if 'optimizer' in study.best_params:\n",
    "    optimizer_name = study.best_params['optimizer']\n",
    "    optimizer = getattr(optim, optimizer_name)(best_model.parameters(), lr=study.best_params['lr'])\n",
    "else:\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=study.best_params['lr'])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    best_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.view(data.size(0), 1, 100, 100).to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = best_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Print training progress\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} completed.\")\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(best_model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# test inference speed\n",
    "import time\n",
    "\n",
    "fake_image = torch.randn(1, 1, 100, 100).to(DEVICE)\n",
    "\n",
    "# warm up\n",
    "for _ in range(10):\n",
    "    _ = best_model(fake_image)\n",
    "\n",
    "start_time = time.time()\n",
    "n_inferences = 1000\n",
    "for _ in range(n_inferences):\n",
    "    _ = best_model(fake_image)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Average inference time: {(end_time - start_time) / n_inferences * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display confidence of a test image using best model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDnFMg90QyL3/H0zRsHI00",
   "gpuType": "T4",
   "mount_file_id": "1xJmP1lHd-OxgeB-glZY4UZ1VX0fjSeNu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
